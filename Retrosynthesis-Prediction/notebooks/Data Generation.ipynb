{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES Augmented Data Generation\n",
    "\n",
    "The core dataset used for these tests was generated by [Liu et al](https://pubs.acs.org/doi/10.1021/acscentsci.7b00303) and is available at [here](https://github.com/pandegroup/reaction_prediction_seq2seq/tree/master/processed_data).\n",
    "\n",
    "The function used for SMILES enumeration was taken from [this repo](https://github.com/EBjerrum/SMILES-enumeration) with minor adaptations for Python3 compatibility.\n",
    "\n",
    "The code below generates three augmented datasets with 4x, 16x and 40x augmentation over the original dataset.\n",
    "\n",
    "The the baseline dataset and the generated augmented datasets can be downloaded [here](https://www.dropbox.com/s/ze4bdif8sqjx5jx/Retrosynthesis%20Data.zip?dl=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SmilesEnumerator import SmilesEnumerator\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Augmentation\n",
    "\n",
    "This script generated four versions of each input datapoint. Specifically, one augmented source sequence and one augmented target sequence will be generated and combined with the un-augmented source and target sequences:\n",
    "\n",
    "`source + target\n",
    "source_augmented + target\n",
    "source + target_augmented\n",
    "source_augmented + target_augmented`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('./USPTO-50K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallAug():\n",
    "    def __init__(self, path):\n",
    "        train_source = list(open(path/'src/src-train.txt'))\n",
    "        train_targs = list(open(path/'tgt/tgt-train.txt'))\n",
    "        self.data = list(zip(train_source, train_targs))\n",
    "        \n",
    "    def augment(self, save_folder_path):\n",
    "        self.generate_augs(self.data)\n",
    "        self.save_df(save_folder_path)\n",
    "        \n",
    "    def generate_augs(self, data):\n",
    "        with ThreadPoolExecutor(8) as ex:\n",
    "            new_data = ex.map(lambda x: self.augment_rxn(x), data)\n",
    "            \n",
    "        aug_data = list(new_data)\n",
    "        self.df = pd.DataFrame(columns=['Source', 'Target', 'rxn_number'])\n",
    "        \n",
    "        for i in range(len(aug_data)):\n",
    "            df_i = pd.DataFrame(aug_data[i], columns=['Source', 'Target'])\n",
    "            df_i['rxn_number'] = i\n",
    "            self.df = self.df.append(df_i)\n",
    "            \n",
    "        self.df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    def save_df(self, path):\n",
    "        self.df.to_csv(path/'augmented_df.csv', index=False)\n",
    "        \n",
    "        sources_aug = list(self.df.Source.values)\n",
    "        with open(path/'train_sources_augmented.txt', 'w') as f:\n",
    "            for sa in sources_aug:\n",
    "                rxn, smile = sa.split(' ')\n",
    "                smile_tok = ' '.join([i for i in smile])\n",
    "                f.write(rxn + ' ' + smile_tok + '\\n')\n",
    "                \n",
    "        targets_aug = list(self.df.Target.values)\n",
    "        with open(path/'train_targets_augmented.txt', 'w') as f:\n",
    "            for ta in targets_aug:\n",
    "                smile_tok = ' '.join([i for i in ta])\n",
    "                f.write(smile_tok + '\\n')\n",
    "                \n",
    "        \n",
    "    def augment_rxn(self, data):\n",
    "        source = data[0].strip('\\n')\n",
    "        targ = data[1].strip('\\n')\n",
    "\n",
    "        sme = SmilesEnumerator()\n",
    "        new_data = []\n",
    "\n",
    "        rxn_class = source.split(' ')[0]\n",
    "\n",
    "        source_smile = ''.join(source.split('> ')[1].split(' '))\n",
    "        targ_smile = ''.join(targ.split(' '))\n",
    "\n",
    "        source_aug = rxn_class + ' ' + sme.randomize_smiles(source_smile)\n",
    "        source_smile = rxn_class + ' ' + source_smile\n",
    "        targ_aug = sme.randomize_smiles(targ_smile)\n",
    "\n",
    "        new_data = [[source_smile, targ_smile],\n",
    "                    [source_aug, targ_smile],\n",
    "                    [source_aug, targ_aug],\n",
    "                    [source_smile, targ_aug]]\n",
    "\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SmallAug(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.augment(path/'augmenteed_data_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium Augmentation\n",
    "\n",
    "The logic here is similar to above. This time we pass an `n_augs` parameter that controls the number of augmented datapoints generated. The class generates `n_augs` augmented datapoints plus the original datapoint for a total of `n_augs + 1` datapoints.\n",
    "\n",
    "For medium augmentation we use `n_augs = 15`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MediumAug():\n",
    "    def __init__(self, path, n_augs):\n",
    "        train_source = list(open(path/'processed_data/train_sources'))\n",
    "        train_targs = list(open(path/'processed_data/train_targets'))\n",
    "        self.data = list(zip(train_source, train_targs))\n",
    "        self.n_augs = n_augs\n",
    "        \n",
    "    def augment(self, save_folder_path):\n",
    "        self.generate_augs(self.data)\n",
    "        self.save_df(save_folder_path)\n",
    "        \n",
    "    def generate_augs(self, data):\n",
    "        with ThreadPoolExecutor(8) as ex:\n",
    "            new_data = ex.map(lambda x: self.augment_rxn(x), data)\n",
    "            \n",
    "        aug_data = list(new_data)\n",
    "        self.df = pd.DataFrame(columns=['Source', 'Target', 'rxn_number'])\n",
    "        \n",
    "        for i in range(len(aug_data)):\n",
    "            df_i = pd.DataFrame(aug_data[i], columns=['Source', 'Target'])\n",
    "            df_i['rxn_number'] = i\n",
    "            self.df = self.df.append(df_i)\n",
    "            \n",
    "        self.df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    def save_df(self, path):\n",
    "        self.df.to_csv(path/'augmented_df.csv', index=False)\n",
    "        \n",
    "        sources_aug = list(self.df.Source.values)\n",
    "        with open(path/'train_sources_augmented.txt', 'w') as f:\n",
    "            for sa in sources_aug:\n",
    "                rxn, smile = sa.split(' ')\n",
    "                smile_tok = ' '.join([i for i in smile])\n",
    "                f.write(rxn + ' ' + smile_tok + '\\n')\n",
    "                \n",
    "        targets_aug = list(self.df.Target.values)\n",
    "        with open(path/'train_targets_augmented.txt', 'w') as f:\n",
    "            for ta in targets_aug:\n",
    "                smile_tok = ' '.join([i for i in ta])\n",
    "                f.write(smile_tok + '\\n')\n",
    "\n",
    "    \n",
    "    def augment_rxn(self, data):\n",
    "        source = data[0].strip('\\n')\n",
    "        targ = data[1].strip('\\n')\n",
    "\n",
    "        sme = SmilesEnumerator()\n",
    "        new_data = []\n",
    "\n",
    "        rxn_class = source.split(' ')[0]\n",
    "\n",
    "        source_smile = ''.join(source.split('> ')[1].split(' '))\n",
    "        targ_smile = ''.join(targ.split(' '))\n",
    "\n",
    "        source_aug = [rxn_class + ' ' + sme.randomize_smiles(source_smile) for i in range(self.n_augs)]\n",
    "        source_aug += [rxn_class + ' ' + source_smile]\n",
    "\n",
    "        targ_aug = [sme.randomize_smiles(targ_smile) for i in range(15)]\n",
    "        targ_aug += [targ_smile]\n",
    "\n",
    "        new_data = [[s,t] for s,t in zip(source_aug, targ_aug)]\n",
    "\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma = MediumAug(path, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma.augment(path/'augmenteed_data_medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Augmentation\n",
    "\n",
    "Larger scale augmentation requires a little extra logic. When generating a large number of augmented sequences randomly, there is a good chance the same augmented variant will appear more than once. This class generates `4 * n_aug` augmented variants, reduces the set of augmented SMILES to a unique set, then pulls `n_aug` samples from the unique set.\n",
    "\n",
    "For large augmentation, we use `n_augs = 40`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeAug():\n",
    "    def __init__(self, path, n_augs):\n",
    "        train_source = list(open(path/'processed_data/train_sources'))\n",
    "        train_targs = list(open(path/'processed_data/train_targets'))\n",
    "        self.data = list(zip(train_source, train_targs))\n",
    "        self.n_augs = n_augs\n",
    "        \n",
    "    def augment(self, save_folder_path):\n",
    "        self.generate_augs(self.data)\n",
    "        self.save_df(save_folder_path)\n",
    "        \n",
    "    def generate_augs(self, data):\n",
    "        with ThreadPoolExecutor(8) as ex:\n",
    "            new_data = ex.map(lambda x: self.augment_rxn(x), data)\n",
    "            \n",
    "        aug_data = list(new_data)\n",
    "        self.df = pd.DataFrame(columns=['Source', 'Target', 'rxn_number'])\n",
    "        \n",
    "        for i in range(len(aug_data)):\n",
    "            df_i = pd.DataFrame(aug_data[i], columns=['Source', 'Target'])\n",
    "            df_i['rxn_number'] = i\n",
    "            self.df = self.df.append(df_i)\n",
    "            \n",
    "        self.df.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "    def save_df(self, path):\n",
    "        self.df.to_csv(path/'augmented_df.csv', index=False)\n",
    "        \n",
    "        sources_aug = list(self.df.Source.values)\n",
    "        with open(path/'train_sources_augmented.txt', 'w') as f:\n",
    "            for sa in sources_aug:\n",
    "                rxn, smile = sa.split(' ')\n",
    "                smile_tok = ' '.join([i for i in smile])\n",
    "                f.write(rxn + ' ' + smile_tok + '\\n')\n",
    "                \n",
    "        targets_aug = list(self.df.Target.values)\n",
    "        with open(path/'train_targets_augmented.txt', 'w') as f:\n",
    "            for ta in targets_aug:\n",
    "                smile_tok = ' '.join([i for i in ta])\n",
    "                f.write(smile_tok + '\\n')\n",
    "\n",
    "    \n",
    "    def augment_rxn(self, data):\n",
    "        source = data[0].strip('\\n')\n",
    "        targ = data[1].strip('\\n')\n",
    "        augs = self.n_augs * 4\n",
    "\n",
    "        sme = SmilesEnumerator()\n",
    "        new_data = []\n",
    "\n",
    "        rxn_class = source.split(' ')[0]\n",
    "\n",
    "        source_smile = ''.join(source.split('> ')[1].split(' '))\n",
    "        targ_smile = ''.join(targ.split(' '))\n",
    "\n",
    "        source_aug = list(set([sme.randomize_smiles(source_smile) for i in range(augs)]))\n",
    "\n",
    "        if len(source_aug) > self.n_augs:\n",
    "            source_aug = random.sample(source_aug, self.n_augs)\n",
    "\n",
    "        targ_aug = list(set([sme.randomize_smiles(targ_smile) for i in range(augs)]))\n",
    "\n",
    "        if len(targ_aug) > self.n_augs:\n",
    "            targ_aug = random.sample(targ_aug, self.n_augs)\n",
    "\n",
    "        source_aug = [rxn_class + ' ' + i for i in source_aug]\n",
    "\n",
    "        for s, t in zip(source_aug, targ_aug):\n",
    "            new_data.append([s,t])\n",
    "\n",
    "        return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "la = LargeAug(path, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.augment(path/'augmenteed_data_large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
